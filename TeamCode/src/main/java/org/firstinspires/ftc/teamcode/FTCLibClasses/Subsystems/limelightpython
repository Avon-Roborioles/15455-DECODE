import cv2
import numpy as np
import math as Math

# global variables go here:
testVar = 0
distToGround=10
degreeAngleToGround=45
PI = 3.1415926
crossHairPoseOnGroundX = 3
crossHairPoseOnGroundY = 17



# runPipeline() is called every frame by Limelight's backend.
def runPipeline(image, llrobot):
    img_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    img_threshold = cv2.inRange(img_hsv, (19, 233, 114), (85, 255, 255))

    contours, _ = cv2.findContours(img_threshold,
    cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    largestContour = np.array([[]])
    llpython = [0,0,0,0,0,0,0,0]
    robX= llrobot[0]
    robY=llrobot[1]
    theta =llrobot[2]
    extendMotorPosition = llrobot[3]
    extendMotorTicksPerInch = llrobot[4]
    eligibleContours = []
    eligiblePoses = []
    print("Go")
    if len(contours)>0:
        for contour in contours:
            x,y,w,h = cv2.boundingRect(contour)
            llpython = [1,x,y,w,h,9,8,7]
            results = cvtFromImgtoCrosshair(x+w/2,y+h/2)
            pose = findLoc(robX,robY,theta,extendMotorPosition,50,results[0],results[1])
            xx = pose[0]
            yy=pose[1]

            insubmersible = xx>30 and xx<70
            ycond = not ((yy< -20 and yy<-23)and (yy<-31))
            print(str((insubmersible and ycond) or not insubmersible))
            if (insubmersible and ycond) or not insubmersible:
                eligibleContours.append(contour)
                eligiblePoses.append(pose)

    if len(eligibleContours) > 0:
        cv2.drawContours(image, eligibleContours, -1, 255, 2)
        largestContour = max(eligibleContours, key=cv2.contourArea)
        x,y,w,h = cv2.boundingRect(largestContour)

        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,0),2)

        results = cvtFromImgtoCrosshair(x+w/2,y+h/2)
        poseindex = 0;
        for i in range(len(eligibleContours)):
            if largestContour is eligibleContours[i]:
                poseindex=i
        pose = eligiblePoses[poseindex]
        print(pose[0])
        llpython[0]=pose[0]
        llpython[1]=pose[1]



    # make sure to return a contour,
    # an image to stream,
    # and optionally an array of up to 8 values for the "llpython"
    # networktables array
    return largestContour, image, llpython
def findLoc(x,y,theta,extendMotorPosition,extendMotorTicksPerInch,resultX,resultY):
    xDist = distToGround*Math.tan(Math.radians(resultX));

    groundDistYAngle = degreeAngleToGround-resultY;

    ''' Use Law of Sines:
    opposite angle of dist to ground and dist to ground
    ty and y dist

    ydist/sin(ty) = distToGround/sin(groundDistYAngle)
    '''

    yDist = Math.sin(Math.radians(resultY))*distToGround/(Math.sin(Math.radians(groundDistYAngle)))


    #Pedro Pathing Axes
    botX = yDist+(crossHairPoseOnGroundX)+ extendMotorPosition/ extendMotorTicksPerInch
    botY = -(xDist+crossHairPoseOnGroundX)
    heading = theta;

    retPoseX=botX*Math.cos(heading)+botY*Math.cos(heading+Math.pi/2)
    retPoseY=(botY*Math.sin(heading+Math.pi/2)+botX*Math.sin(heading))



#        Switch X and Y and invert the destination Y for Pedro Pathing
    return [retPoseX,retPoseY]



def cvtFromImgtoCrosshair(x,y):
    nx = (1/160) * (x - 159.5)
    ny = (1/120) * (119.5 - y)
    vpw = 2.0*Math.tan(Math.radians(54/2))
    vph = 2.0*Math.tan(Math.radians(41/2))
    ix = vpw/2 * nx
    iy = vph/2 * ny
    ax = Math.atan2(x, 1)
    ay = Math.atan2(y, 1)
    return [ax,ay]